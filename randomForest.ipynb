{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa32fa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "import mahotas\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "# --- Step 1: Load Datasets ---\n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = ImageFolder(root=\"/home/abhivandhana/computerVision/train\", transform=transform)\n",
    "val_dataset = ImageFolder(root=\"/home/abhivandhana/computerVision/val\", transform=transform)\n",
    "\n",
    "print(f\"Training images: {len(train_dataset)}\")\n",
    "print(f\"Validation images: {len(val_dataset)}\")\n",
    "\n",
    "def get_image_details(dataset):\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    images, _ = next(iter(loader))\n",
    "    c, h, w = images.shape[1:]\n",
    "    print(f\"Image size: {h}x{w}, Channels: {c}\")\n",
    "\n",
    "print(\"\\nTraining Image Details:\")\n",
    "get_image_details(train_dataset)\n",
    "print(\"\\nValidation Image Details:\")\n",
    "get_image_details(val_dataset)\n",
    "\n",
    "# --- Step 2: Class distribution ---\n",
    "train_dataset = ImageFolder(root=\"/home/abhivandhana/Documents/life/PG/cv/project/archive (2)/train_images/dataset/train\")\n",
    "val_dataset = ImageFolder(root=\"/home/abhivandhana/Documents/life/PG/cv/project/archive (2)/train_images/dataset/val\")\n",
    "print(\"Train class distribution:\", Counter(train_dataset.targets))\n",
    "print(\"Val class distribution:\", Counter(val_dataset.targets))\n",
    "\n",
    "# --- Step 3: Feature Extraction ---\n",
    "image_folder = \"/home/abhivandhana/Documents/life/PG/cv/project/train_images/train_images\"\n",
    "features = []\n",
    "\n",
    "for filename in os.listdir(image_folder):\n",
    "    if filename.lower().endswith((\".jpg\", \".png\")):\n",
    "        img_path = os.path.join(image_folder, filename)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        mean_intensity = np.mean(img)\n",
    "        std_intensity = np.std(img)\n",
    "        glcm = graycomatrix(img, [5], [0], 256, symmetric=True, normed=True)\n",
    "        contrast = graycoprops(glcm, 'contrast')[0, 0]\n",
    "        homogeneity = graycoprops(glcm, 'homogeneity')[0, 0]\n",
    "        haralick = mahotas.features.haralick(img).mean()\n",
    "\n",
    "        blurred = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "        _, thresh = cv2.threshold(blurred, 60, 255, cv2.THRESH_BINARY)\n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        max_radius, lesion_area, linearity = 0, 0, 0\n",
    "        if contours:\n",
    "            largest = max(contours, key=cv2.contourArea)\n",
    "            lesion_area = cv2.contourArea(largest)\n",
    "            (_, _), max_radius = cv2.minEnclosingCircle(largest)\n",
    "            perimeter = cv2.arcLength(largest, True)\n",
    "            if lesion_area > 0:\n",
    "                linearity = (perimeter ** 2) / lesion_area\n",
    "\n",
    "        features.append([\n",
    "            filename, mean_intensity, std_intensity, contrast,\n",
    "            homogeneity, haralick, max_radius, lesion_area, linearity\n",
    "        ])\n",
    "\n",
    "columns = [\"File_Name\", \"Mean_Intensity\", \"STD_Intensity\", \"Contrast\", \"Homogeneity\", \"Haralick_Feature\", \"Radius\", \"Area\", \"Linearity\"]\n",
    "df = pd.DataFrame(features, columns=columns)\n",
    "\n",
    "# --- Step 4: Clustering ---\n",
    "feature_columns = columns[1:]\n",
    "X = df[feature_columns]\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "df[\"BCLC_Cluster\"] = kmeans.fit_predict(X)\n",
    "\n",
    "# --- Step 5: Map Clusters to Stages ---\n",
    "cluster_mapping = {\n",
    "    0: \"Stage 0 (Very Early)\",\n",
    "    1: \"Stage A (Early)\",\n",
    "    2: \"Stage B (Intermediate)\",\n",
    "    3: \"Stage C (Advanced)\",\n",
    "    4: \"Stage D (End-Stage)\"\n",
    "}\n",
    "df[\"BCLC_Stage\"] = df[\"BCLC_Cluster\"].map(cluster_mapping)\n",
    "df.to_csv(\"lesion_features_bclc.csv\", index=False)\n",
    "\n",
    "# --- Step 6: Random Forest Classification ---\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"BCLC_Stage_Label\"] = label_encoder.fit_transform(df[\"BCLC_Stage\"])\n",
    "X = df[feature_columns]\n",
    "y = df[\"BCLC_Stage_Label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# --- Step 7: Confusion Matrix ---\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# --- Step 8: Neural Network ---\n",
    "nn_model = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "nn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = nn_model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))\n",
    "\n",
    "# --- Step 9: Plot NN Accuracy & Loss ---\n",
    "plt.plot(np.array(history.history['accuracy']) * 100, label='Train Accuracy')\n",
    "plt.plot(np.array(history.history['val_accuracy']) * 100, label='Val Accuracy')\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"NN Accuracy Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"NN Loss Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# --- Step 10: Predict and Visualize ---\n",
    "random_indices = random.sample(range(len(X_test)), 30)\n",
    "X_sample = X_test.iloc[random_indices]\n",
    "y_sample_actual = y_test.iloc[random_indices]\n",
    "y_sample_pred = rf_model.predict(X_sample)\n",
    "\n",
    "actual_labels = label_encoder.inverse_transform(y_sample_actual)\n",
    "pred_labels = label_encoder.inverse_transform(y_sample_pred)\n",
    "\n",
    "# Attach prediction info\n",
    "sample_df = df.loc[X_sample.index].copy()\n",
    "sample_df[\"Actual_Stage\"] = actual_labels\n",
    "sample_df[\"Predicted_Stage\"] = pred_labels\n",
    "\n",
    "image_dir = \"/home/abhivandhana/Documents/life/PG/cv/project/train_images/train_images\"\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "for i, row in sample_df.iterrows():\n",
    "    img_path = os.path.join(image_dir, row[\"File_Name\"])\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    if img is None:\n",
    "        print(f\"Could not load {img_path}\")\n",
    "        continue\n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_with_text = img.copy()\n",
    "\n",
    "    # Predicted Stage on the image (bottom-left)\n",
    "    cv2.putText(img_with_text,\n",
    "                f\"Predicted: {row['Predicted_Stage']}\",\n",
    "                org=(10, img.shape[0] - 10),\n",
    "                fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                fontScale=0.6,\n",
    "                color=(255, 0, 0),  # Blue\n",
    "                thickness=2,\n",
    "                lineType=cv2.LINE_AA)\n",
    "\n",
    "    # Plotting\n",
    "    plt.subplot(5, 6, list(sample_df.index).index(i) + 1)\n",
    "    plt.imshow(img_with_text)\n",
    "    plt.title(f\"Actual: {row['Actual_Stage']}\", fontsize=8)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae87f88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
